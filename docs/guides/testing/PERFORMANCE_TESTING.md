# Performance Testing Guide

## ğŸ“Š Overview

Comprehensive performance testing framework using three industry-standard tools, each serving different purposes and scenarios.

---

## ğŸ¯ Tool Allocation

| Tool | Allocation | Purpose | Language | Strength |
|------|------------|---------|----------|----------|
| **Locust** | **40%** | Primary load testing | Python | Real-time UI, flexible scripting |
| **Gatling** | **30%** | Detailed analysis | Scala | Beautiful reports, metrics |
| **JMeter** | **30%** | Industry standard | Java | Protocol support, mature |

---

## ğŸ Locust (40% - Primary Tool)

### Why Locust is Primary

**Advantages:**
- âœ… Real-time web UI dashboard
- âœ… Python-based (easy to learn)
- âœ… Flexible task weighting
- âœ… Distributed load testing
- âœ… Event hooks for custom logic
- âœ… Great for API and web testing

### Installation

```bash
# Install Python dependencies
pip install -r requirements.txt

# Or install Locust directly
pip install locust==2.20.0
```

### Test Files

**1. api_load_test.py**
- API-focused load testing
- CRUD operations
- Weighted tasks
- Realistic user behavior

**2. web_load_test.py**
- Website load testing
- Multiple site navigation
- Page load metrics

**3. comprehensive_load_test.py**
- Complete API coverage
- Sequential user journeys
- Batch request patterns
- Custom metrics

### Running Locust

**Interactive Mode (Recommended):**
```bash
./scripts/run-locust-tests.sh
# Then open: http://localhost:8089
# Configure users and spawn rate in web UI
```

**Headless Mode (CI/CD):**
```bash
locust -f src/test/locust/api_load_test.py \
       --headless \
       --users 100 \
       --spawn-rate 10 \
       --run-time 3m \
       --html target/locust/report.html \
       --csv target/locust/stats
```

**Distributed Mode (Multiple Machines):**
```bash
# Master
locust -f src/test/locust/api_load_test.py --master

# Workers (on other machines)
locust -f src/test/locust/api_load_test.py --worker --master-host=<master-ip>
```

### Metrics Collected

- **Requests per second (RPS)**
- **Response time** (min/max/avg/p50/p90/p95/p99)
- **Failure rate**
- **Number of users**
- **Request distribution**
- **Custom metrics** (via events)

### Reports

- **Web UI**: Real-time metrics at http://localhost:8089
- **HTML Report**: `target/locust/report.html`
- **CSV Stats**: `target/locust/stats_*.csv`
- **Charts**: Downloadable from web UI

---

## âš¡ Gatling (30% - Detailed Analysis)

### Why Gatling

**Advantages:**
- âœ… Beautiful HTML reports
- âœ… Detailed metrics and graphs
- âœ… High performance (async I/O)
- âœ… Scenario DSL
- âœ… JVM-based (integrates with Maven)
- âœ… Great for complex scenarios

### Test Files

**1. ApiLoadSimulation.scala**
- API endpoint testing
- Ramp-up patterns
- Assertion validation
- Throughput testing

**2. WebLoadSimulation.scala**
- Website load testing
- Page performance
- Multi-site scenarios

### Running Gatling

**All Simulations:**
```bash
./scripts/run-gatling-tests.sh
```

**With Maven (requires -Pgatling profile):**
```bash
./mvnw gatling:test -Pgatling
```

**Specific Simulation:**
```bash
./mvnw gatling:test -Pgatling -Dgatling.simulationClass=simulations.ApiLoadSimulation
```

**With Custom Users:**
```bash
./mvnw gatling:test -Pgatling -Dgatling.simulationClass=simulations.ApiLoadSimulation \
                    -Dusers=100
```

**Note:** The `-Pgatling` profile activates Scala compilation. This is disabled by default to avoid compiling Scala files during normal builds (since they're not needed for UI/API tests).

### Load Profiles

**ApiLoadSimulation:**
```
- Ramp: 1 to 50 users over 30 seconds
- Sustain: 50 users for 60 seconds
- Assertions:
  - Max response time < 5s
  - Mean response time < 1s
  - Success rate > 95%
```

**WebLoadSimulation:**
```
- Ramp: 1 to 30 users over 20 seconds
- Sites: Google, GitHub, Wikipedia, W3C
- Assertions:
  - Max response time < 10s
  - Mean response time < 3s
  - Success rate > 90%
```

### Reports

- **HTML Dashboard**: `target/gatling/<simulation-timestamp>/index.html`
- **Global Stats**: Response time distribution, RPS, percentiles
- **Request Stats**: Per-request breakdown
- **Active Users Over Time**: Graph
- **Response Time Distribution**: Histogram
- **Percentiles Over Time**: p95, p99 graphs

---

## ğŸ“Š JMeter (30% - Industry Standard)

### Why JMeter

**Advantages:**
- âœ… Industry standard
- âœ… Comprehensive protocol support
- âœ… GUI test builder
- âœ… Extensive plugins
- âœ… Corporate-friendly
- âœ… Mature ecosystem

### Test Plans

**1. API_Performance_Test.jmx**
- REST API testing
- GET/POST operations
- Response assertions
- 50 concurrent users

**2. Web_Load_Test.jmx**
- Website load testing
- Multiple pages
- 30 concurrent users
- Response time validation

### Running JMeter

**Via Maven:**
```bash
./scripts/run-jmeter-tests.sh
```

**Direct Maven:**
```bash
# Run tests
./mvnw jmeter:jmeter

# Generate reports
./mvnw jmeter:results
```

**JMeter GUI (Test Creation):**
```bash
# Download JMeter from apache.org
jmeter -t src/test/jmeter/API_Performance_Test.jmx
```

### Configuration

**Test Plans (.jmx files):**
- Thread Groups (users/ramp-up)
- HTTP Samplers (requests)
- Assertions (validation)
- Timers (think time)
- Listeners (results)

**Maven Plugin:**
```xml
<plugin>
    <groupId>com.lazerycode.jmeter</groupId>
    <artifactId>jmeter-maven-plugin</artifactId>
    <configuration>
        <testFilesDirectory>src/test/jmeter</testFilesDirectory>
        <resultsDirectory>target/jmeter/results</resultsDirectory>
        <generateReports>true</generateReports>
    </configuration>
</plugin>
```

### Reports

- **HTML Dashboard**: `target/jmeter/reports/index.html`
- **JTL Results**: `target/jmeter/results/*.jtl`
- **CSV Stats**: Imported into Excel/tools
- **Graphs**: Response time, throughput, errors

---

## ğŸ“ˆ Performance Metrics

### Standard Metrics (All Tools)

**Response Time:**
- Minimum
- Maximum
- Average
- Median (p50)
- 95th percentile (p95)
- 99th percentile (p99)

**Throughput:**
- Requests per second (RPS)
- Transactions per second (TPS)
- Data transferred (bytes/sec)

**Reliability:**
- Success rate (%)
- Error rate (%)
- Error types

**Concurrency:**
- Active users
- User spawn rate
- Session duration

### Tool-Specific Metrics

**Locust:**
- Request distribution by task
- Failure reasons
- Custom metrics via events

**Gatling:**
- Active users over time graph
- Response time distribution
- Request count per second
- Percentile graphs

**JMeter:**
- Transaction times
- Connect times
- Latency
- Thread statistics

---

## ğŸ¯ Test Scenarios

### API Load Testing

**Target:** JSONPlaceholder API (https://jsonplaceholder.typicode.com)

**Endpoints Tested:**
- GET /posts - List all posts
- GET /posts/{id} - Get specific post
- POST /posts - Create post
- PUT /posts/{id} - Update post
- DELETE /posts/{id} - Delete post
- GET /users - List users
- GET /comments - List comments

**Load Pattern:**
- Ramp up gradually
- Sustain peak load
- Measure stability

### Web Load Testing

**Targets:**
- Google (search engine)
- GitHub (developer platform)
- Wikipedia (content site)
- W3C (standards site)

**Patterns:**
- Homepage access
- Navigation patterns
- Concurrent browsing
- Page load times

---

## ğŸš€ Quick Start

### Run Individual Tools

```bash
# Locust (Interactive)
./scripts/run-locust-tests.sh

# Gatling
./scripts/run-gatling-tests.sh

# JMeter
./scripts/run-jmeter-tests.sh
```

### Run All Tools

```bash
# Sequential execution
./scripts/run-all-performance-tests.sh
```

### CI/CD Integration

```bash
# Headless execution for automation
locust -f src/test/locust/api_load_test.py --headless \
       --users 50 --spawn-rate 5 --run-time 2m \
       --html target/locust/report.html

./mvnw gatling:test
./mvnw jmeter:jmeter jmeter:results
```

---

## ğŸ“Š Comparing Results

### Response Time Comparison

| Tool | API (avg) | Web (avg) | Format |
|------|-----------|-----------|--------|
| Locust | Real-time | Real-time | Web UI + HTML |
| Gatling | Post-test | Post-test | HTML Dashboard |
| JMeter | Post-test | Post-test | HTML + CSV |

### Use Cases

**Use Locust when:**
- âœ… Need real-time monitoring
- âœ… Want Python flexibility
- âœ… Testing APIs
- âœ… Need distributed testing

**Use Gatling when:**
- âœ… Need detailed reports
- âœ… Want scenario DSL
- âœ… Testing complex flows
- âœ… Need high throughput

**Use JMeter when:**
- âœ… Corporate requirements
- âœ… Need GUI test builder
- âœ… Testing multiple protocols
- âœ… Integration with CI/CD tools

---

## ğŸ“ Best Practices

### Load Profile Design

**1. Ramp-Up:**
```
Don't: Start with 1000 users immediately
Do:    Ramp from 1 to 1000 over 60 seconds
```

**2. Think Time:**
```
Don't: No pauses between requests
Do:    1-5 second pauses (realistic behavior)
```

**3. Duration:**
```
Smoke Test:     1-2 minutes
Load Test:      5-10 minutes
Stress Test:    30+ minutes
Soak Test:      Hours
```

### Realistic Scenarios

**API Testing:**
- Mix of read/write operations (80/20 rule)
- Weighted tasks (common operations more frequent)
- Error handling
- Timeout configurations

**Web Testing:**
- Page navigation patterns
- Think time between pages
- Cache behavior
- Resource loading

### Interpreting Results

**Good Performance:**
- âœ… p95 response time < 1 second
- âœ… Success rate > 99%
- âœ… Stable throughput
- âœ… No errors under load

**Warning Signs:**
- âš ï¸  Increasing response times
- âš ï¸  Error rate > 1%
- âš ï¸  Declining throughput
- âš ï¸  Timeouts

**Critical Issues:**
- âŒ p95 > 5 seconds
- âŒ Success rate < 95%
- âŒ Errors > 5%
- âŒ System crashes

---

## ğŸ“ File Structure

```
src/test/
â”œâ”€â”€ scala/                    # Gatling simulations
â”‚   â”œâ”€â”€ ApiLoadSimulation.scala
â”‚   â””â”€â”€ WebLoadSimulation.scala
â”‚
â”œâ”€â”€ jmeter/                   # JMeter test plans
â”‚   â”œâ”€â”€ API_Performance_Test.jmx
â”‚   â””â”€â”€ Web_Load_Test.jmx
â”‚
â””â”€â”€ locust/                   # Locust tests
    â”œâ”€â”€ api_load_test.py
    â”œâ”€â”€ web_load_test.py
    â””â”€â”€ comprehensive_load_test.py

scripts/
â”œâ”€â”€ run-locust-tests.sh       # Locust runner
â”œâ”€â”€ run-gatling-tests.sh      # Gatling runner
â”œâ”€â”€ run-jmeter-tests.sh       # JMeter runner
â””â”€â”€ run-all-performance-tests.sh  # Master runner

target/
â”œâ”€â”€ locust/                   # Locust results
â”‚   â”œâ”€â”€ report.html
â”‚   â””â”€â”€ stats*.csv
â”‚
â”œâ”€â”€ gatling/                  # Gatling results
â”‚   â””â”€â”€ <simulation>-<timestamp>/
â”‚       â””â”€â”€ index.html
â”‚
â””â”€â”€ jmeter/                   # JMeter results
    â”œâ”€â”€ results/*.jtl
    â””â”€â”€ reports/index.html
```

---

## ğŸ”§ Configuration

### Locust Configuration

**In test file:**
```python
class ApiUser(HttpUser):
    wait_time = between(1, 3)  # Wait time range
    host = "https://api.example.com"

    @task(10)  # Weight: appears 10 times more often
    def common_task(self):
        pass
```

**Command line:**
```bash
locust -f test.py \
       --users 100 \          # Total users
       --spawn-rate 10 \      # Users/sec to spawn
       --run-time 5m \        # Duration
       --host https://api.example.com
```

### Gatling Configuration

**In simulation:**
```scala
setUp(
  scenario.inject(
    rampUsers(100).during(60.seconds),
    constantUsersPerSec(10).during(5.minutes)
  )
).assertions(
  global.responseTime.max.lt(5000),
  global.successfulRequests.percent.gt(95)
)
```

### JMeter Configuration

**In .jmx file (GUI):**
- Thread Group: Users, ramp-up, loops
- HTTP Sampler: Method, path, body
- Assertions: Response code, content
- Timers: Think time

---

## ğŸ“Š Reports

### Locust Reports

**Web UI (http://localhost:8089):**
- Real-time statistics
- Active users graph
- Response time graphs
- Request distribution
- Failure breakdown
- Download data (CSV)

**HTML Report:**
```bash
target/locust/report.html
```
- Summary statistics
- Charts and graphs
- Request breakdown
- Timeline

### Gatling Reports

**HTML Dashboard:**
```bash
target/gatling/<simulation>-<timestamp>/index.html
```

**Sections:**
- Global statistics
- Request statistics
- Active users over time
- Response time distribution
- Response time percentiles
- Requests per second

### JMeter Reports

**HTML Dashboard:**
```bash
target/jmeter/reports/index.html
```

**Sections:**
- Summary report
- Response time graph
- Throughput graph
- Error report
- Top 5 errors

---

## ğŸ¯ Usage Examples

### Example 1: Quick API Load Test

```bash
# Using Locust (fastest)
locust -f src/test/locust/api_load_test.py \
       --headless \
       --users 50 \
       --spawn-rate 5 \
       --run-time 1m
```

**Results in ~1 minute:**
- Request stats
- Response times
- Success/failure rates

### Example 2: Comprehensive Analysis

```bash
# Run all three tools
./scripts/run-all-performance-tests.sh
```

**Compares:**
- Locust results (real-time behavior)
- Gatling results (detailed analysis)
- JMeter results (standard metrics)

### Example 3: Stress Testing

```bash
# Locust - Find breaking point
locust -f src/test/locust/comprehensive_load_test.py
# Manually increase users until failures occur
```

### Example 4: Soak Testing

```bash
# Long-running stability test
locust -f src/test/locust/api_load_test.py \
       --headless \
       --users 20 \
       --spawn-rate 2 \
       --run-time 2h \
       --html target/locust/soak-test.html
```

---

## ğŸ› Troubleshooting

### Locust Issues

**Problem: "Connection refused"**
```bash
# Check if target is accessible
curl https://jsonplaceholder.typicode.com/posts

# Check firewall/network
```

**Problem: "Too many open files"**
```bash
# macOS/Linux: Increase file descriptor limit
ulimit -n 10000
```

### Gatling Issues

**Problem: "Compilation error"**
```bash
# Ensure Scala plugin is working
./mvnw scala:compile

# Check Scala syntax
```

**Problem: "OutOfMemoryError"**
```bash
# Increase Maven memory
export MAVEN_OPTS="-Xmx4g"
./mvnw gatling:test
```

### JMeter Issues

**Problem: "Test plan not found"**
```bash
# Verify .jmx files exist
ls -la src/test/jmeter/*.jmx
```

**Problem: "No results generated"**
```bash
# Run with debug
./mvnw jmeter:jmeter -X
```

---

## ğŸ“ˆ Performance Testing Strategy

### Layered Approach

**Level 1: Smoke Test (1 min)**
```bash
locust -f api_load_test.py --headless --users 10 --run-time 1m
```
- Quick validation
- Basic functionality
- Run before commit

**Level 2: Load Test (5 min)**
```bash
./scripts/run-locust-tests.sh  # 100 users, 3 min
```
- Expected production load
- Sustained performance
- Run before deployment

**Level 3: Stress Test (15 min)**
```bash
./scripts/run-all-performance-tests.sh
```
- All three tools
- High load scenarios
- Find breaking points
- Run weekly

**Level 4: Soak Test (hours)**
```bash
locust --users 50 --run-time 4h
```
- Long-duration stability
- Memory leak detection
- Resource exhaustion
- Run monthly

---

## ğŸ¯ Key Metrics to Monitor

### Response Time

**Targets:**
- API calls: < 500ms (avg)
- Web pages: < 2s (avg)
- p95: < 1s (API), < 5s (Web)
- p99: < 2s (API), < 10s (Web)

### Throughput

**Targets:**
- API: 100+ RPS
- Web: 20+ RPS
- Stable under load
- No degradation over time

### Error Rate

**Targets:**
- < 0.1% in normal conditions
- < 1% under stress
- 0% for critical operations

### Resource Utilization

**Monitor:**
- CPU usage
- Memory usage
- Network bandwidth
- Connection pool

---

## ğŸš€ CI/CD Integration

### GitHub Actions (Future)

```yaml
performance-tests:
  runs-on: ubuntu-latest
  steps:
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Locust
      run: pip install -r requirements.txt

    - name: Run Locust Tests
      run: |
        locust -f src/test/locust/api_load_test.py \
               --headless \
               --users 50 \
               --spawn-rate 5 \
               --run-time 2m \
               --html target/locust/report.html

    - name: Upload Results
      uses: actions/upload-artifact@v4
      with:
        name: performance-results
        path: target/locust/
```

---

## ğŸ“š Resources

### Locust
- Docs: https://docs.locust.io/
- Examples: https://github.com/locustio/locust/tree/master/examples
- Plugins: https://github.com/SvenskaSpel/locust-plugins

### Gatling
- Docs: https://gatling.io/docs/
- DSL Reference: https://gatling.io/docs/gatling/reference/current/core/
- Examples: https://github.com/gatling/gatling-maven-plugin-demo

### JMeter
- Docs: https://jmeter.apache.org/usermanual/
- Best Practices: https://jmeter.apache.org/usermanual/best-practices.html
- Plugins: https://jmeter-plugins.org/

---

## âœ… Quick Reference

| Task | Command |
|------|---------|
| **Run Locust (Interactive)** | `./scripts/run-locust-tests.sh` |
| **Run Locust (Headless)** | `locust -f src/test/locust/api_load_test.py --headless --users 100 --run-time 2m` |
| **Run Gatling** | `./scripts/run-gatling-tests.sh` |
| **Run JMeter** | `./scripts/run-jmeter-tests.sh` |
| **Run All** | `./scripts/run-all-performance-tests.sh` |
| **Install Locust** | `pip install -r requirements.txt` |
| **View Locust UI** | http://localhost:8089 |
| **Gatling Reports** | `target/gatling/*/index.html` |
| **JMeter Reports** | `target/jmeter/reports/index.html` |

---

## ğŸ“ Tips

1. **Start Small**: Begin with 10 users, increase gradually
2. **Monitor Resources**: Watch CPU/memory on target
3. **Realistic Scenarios**: Model actual user behavior
4. **Think Time**: Add pauses between requests
5. **Assertions**: Validate responses, not just status codes
6. **Baseline First**: Record normal performance
7. **Incremental Load**: Find the breaking point
8. **Analyze Failures**: Investigate errors immediately
9. **Compare Tools**: Cross-validate with multiple tools
10. **Document Results**: Track trends over time
