# Adding Performance Tests to CI/CD

## ğŸ“‹ Overview

This guide shows how to integrate performance tests into the GitHub Actions CI/CD pipeline. Currently, performance tests run manually/locally only.

---

## âš ï¸ Considerations

### **Before Adding to CI/CD:**

**Resource Costs:**
- Performance tests consume significant CPU/memory
- GitHub Actions has usage limits
- Could exceed free tier quickly

**Duration:**
- Current CI/CD: ~15-20 minutes
- With performance: ~25-35 minutes
- Slower feedback for developers

**External Dependencies:**
- Tests target jsonplaceholder.typicode.com
- Could be rate-limited
- Could block GitHub IPs

**Recommendation:**
- Run on schedule (weekly) instead of every push
- Use manual trigger (workflow_dispatch)
- Run only on main branch merges

---

## ğŸ¯ Option 1: Scheduled Performance Tests (Recommended)

### **Add to .github/workflows/performance.yml:**

```yaml
name: Performance Tests

on:
  schedule:
    # Run every Sunday at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:  # Allow manual trigger
  push:
    branches:
      - main
    paths:
      - 'src/test/locust/**'
      - 'src/test/scala/**'
      - 'src/test/jmeter/**'

jobs:
  locust-performance:
    name: Locust Performance Tests (40%)
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Locust
        run: pip install -r requirements.txt

      - name: Run Locust Tests
        run: |
          mkdir -p target/locust
          locust -f src/test/locust/api_load_test.py \
                 --headless \
                 --users 100 \
                 --spawn-rate 10 \
                 --run-time 3m \
                 --html target/locust/api-report.html \
                 --csv target/locust/api-stats

      - name: Upload Locust Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: locust-results
          path: target/locust/
          retention-days: 30

  gatling-performance:
    name: Gatling Performance Tests (30%)
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK 21
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'
          cache: 'maven'

      - name: Run Gatling Tests
        run: ./mvnw gatling:test -Pgatling

      - name: Upload Gatling Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: gatling-results
          path: target/gatling/
          retention-days: 30

  jmeter-performance:
    name: JMeter Performance Tests (30%)
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK 21
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'
          cache: 'maven'

      - name: Run JMeter Tests
        run: ./mvnw jmeter:jmeter jmeter:results

      - name: Upload JMeter Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: jmeter-results
          path: target/jmeter/
          retention-days: 30

  performance-summary:
    name: Performance Test Summary
    runs-on: ubuntu-latest
    needs: [locust-performance, gatling-performance, jmeter-performance]
    if: always()

    steps:
      - name: Download All Results
        uses: actions/download-artifact@v4
        with:
          path: performance-results

      - name: Display Summary
        run: |
          echo "ğŸ“Š Performance Test Results Available:"
          echo ""
          echo "Locust Results:"
          ls -lah performance-results/locust-results/ || echo "No results"
          echo ""
          echo "Gatling Results:"
          ls -lah performance-results/gatling-results/ || echo "No results"
          echo ""
          echo "JMeter Results:"
          ls -lah performance-results/jmeter-results/ || echo "No results"
          echo ""
          echo "Download artifacts to view HTML reports!"
```

**Benefits:**
- âœ… Runs weekly automatically
- âœ… Can trigger manually
- âœ… Doesn't slow daily CI/CD
- âœ… Results saved as artifacts

---

## ğŸ¯ Option 2: On-Demand Only (Current Setup)

### **Keep Performance Tests Manual:**

```bash
# Developers run locally as needed
./scripts/run-locust-tests.sh
./scripts/run-gatling-tests.sh
./scripts/run-jmeter-tests.sh
```

**Benefits:**
- âœ… No CI/CD resource usage
- âœ… Fast CI/CD feedback
- âœ… Full control over load testing
- âœ… Can test at any scale

**When to Run:**
- Before major releases
- After performance-critical changes
- Weekly/monthly performance validation
- Capacity planning
- Debugging performance issues

---

## ğŸ¯ Option 3: Hybrid Approach (Recommended)

### **Lightweight Smoke Performance Test in CI/CD:**

```yaml
# Add to existing ci.yml
quick-performance-check:
  name: Quick Performance Check
  runs-on: ubuntu-latest
  needs: build-and-compile
  if: github.ref == 'refs/heads/main'  # Only on main branch

  steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Locust
      run: pip install locust==2.20.0

    - name: Quick API Performance Test
      run: |
        locust -f src/test/locust/api_load_test.py \
               --headless \
               --users 10 \
               --spawn-rate 2 \
               --run-time 30s \
               --html target/locust/quick-check.html
```

**Benefits:**
- âœ… Quick smoke test (30 seconds)
- âœ… Catches major performance regressions
- âœ… Minimal CI/CD impact
- âœ… Only on main branch

**Plus:**
- Full performance tests still run manually
- Comprehensive testing when needed
- Best of both worlds

---

## ğŸ“ˆ **Current Execution Model**

```
Daily Development:
â”œâ”€ Code changes
â”œâ”€ Push to GitHub
â”œâ”€ CI/CD runs automatically
â”‚  â”œâ”€ Smoke Tests (5)
â”‚  â”œâ”€ Grid Tests Chrome (11)
â”‚  â””â”€ Grid Tests Firefox (11)
â””â”€ âœ… All functional tests pass

Weekly/As-Needed:
â”œâ”€ Developer runs locally:
â”‚  â”œâ”€ ./scripts/run-locust-tests.sh
â”‚  â”œâ”€ ./scripts/run-gatling-tests.sh
â”‚  â””â”€ ./scripts/run-jmeter-tests.sh
â””â”€ ğŸ“Š Performance validated
```

---

## âœ… **Summary**

### **Current State:**
- âœ… Performance tests available
- âœ… Run manually/locally
- âœ… Not in CI/CD pipeline
- âœ… On-demand execution

### **To Add to CI/CD:**
1. Create `.github/workflows/performance.yml`
2. Use the examples above
3. Run on schedule or manual trigger
4. Upload results as artifacts

### **Recommended:**
Keep performance tests manual for now because:
- âœ… Faster CI/CD feedback
- âœ… No resource overhead
- âœ… Full flexibility
- âœ… Can add to CI/CD later if needed

---

## ğŸ“ **Quick Reference**

| What | When | How |
|------|------|-----|
| **Functional Tests** | Every push (automatic) | GitHub Actions CI/CD |
| **Performance Tests** | Manual/weekly | Local execution via scripts |
| **API Tests** | Every push (automatic) | Could add to CI/CD |
| **Smoke Tests** | Every push (automatic) | GitHub Actions CI/CD |

**Performance tests are available whenever you need them, just not automated yet!**
